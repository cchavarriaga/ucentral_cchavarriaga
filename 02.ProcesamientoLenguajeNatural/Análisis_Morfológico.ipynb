{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD_pUQRTYi3B"
      },
      "source": [
        "# Análisis morfológico\n",
        "\n",
        "Es un proceso mediante el cual se examinan las partes que constituyen a las palabras. Implica realizar una disección de los elementos que integran la estructura de una palabra, buscando sus elementos más pequeños. Mediante este proceso se separan los lexemas de los morfemas, se indica entonces la raíz y los modificadores que le acompañan respectivamente.  Apunta mucho a la estructura, una suerte de disección del lenguaje para encontrar la relación y sentido de sus componentes esenciales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7w2bdBRYi3G"
      },
      "source": [
        "## Lematización\n",
        "\n",
        "Consiste en la identificación y asignación de la raíz de una palabra a un lema -una forma canónica de la misma- en forma de etiqueta:\n",
        "\n",
        "> «Lema: forma de citación de una palabra (p. ej., el lema de reía es reir).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN4Ao9XeYi3G"
      },
      "source": [
        "Tenemos diversas herramientas en Python para lemmatizar textos:\n",
        "\n",
        "### WordNet\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2N6_odYYi3H",
        "outputId": "3e7fd562-02b2-4880-af0e-b0f6b10fe0a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\cizai\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdEMzf8fYi3I",
        "outputId": "6bb69480-0970-4769-8fc2-ef22d06d4787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "someone\n",
            "think\n",
            "in\n",
            "plural\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "# Inicialización del lematizador de WordNet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "print(lemmatizer.lemmatize(\"someones\"))\n",
        "print(lemmatizer.lemmatize(\"thinks\"))\n",
        "print(lemmatizer.lemmatize(\"in\"))\n",
        "print(lemmatizer.lemmatize(\"plurals\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aB74-POMYi3J",
        "outputId": "58f77051-6a0a-4e02-b9f4-5a54852f55d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'had', 'no', 'idea', 'that', 'such', 'individuals', 'exist', 'outside', 'of', 'stories']\n",
            "I had no idea that such individual exist outside of story\n"
          ]
        }
      ],
      "source": [
        "sentence = \"I had no idea that such individuals exist outside of stories\"\n",
        "\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(word_list)\n",
        "\n",
        "\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "print(lemmatized_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU7gpMddYi3J"
      },
      "source": [
        "Que ocurre aquí:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTa22GF4Yi3K",
        "outputId": "cb6f0384-5bb6-4f67-b03c-918d71d3f72e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lose\n",
            "my\n",
            "religion\n"
          ]
        }
      ],
      "source": [
        "print(lemmatizer.lemmatize(\"lost\", 'v'))  \n",
        "print(lemmatizer.lemmatize(\"my\",'n')) \n",
        "print(lemmatizer.lemmatize(\"religions\",'n')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur_CMWiiYi3L",
        "outputId": "e1386303-662c-4f1f-8f87-bdf0602c25de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lost\n",
            "my\n",
            "religion\n"
          ]
        }
      ],
      "source": [
        "print(lemmatizer.lemmatize(\"lost\",'a'))  \n",
        "print(lemmatizer.lemmatize(\"my\",'n')) \n",
        "print(lemmatizer.lemmatize(\"religions\",'n')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU4gKpZGYi3M"
      },
      "outputs": [],
      "source": [
        "print(lemmatizer.lemmatize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0e6NZKYYi3N"
      },
      "source": [
        "Evidentemente debemos usar un tagged para hacer una buena lematización:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPsOz4nWYi3O"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1ADNjGQYi3O",
        "outputId": "cba70412-9d07-47e8-9320-0826bc0e4c2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'v'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_wordnet_pos('lost')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0f_sAUxYi3P",
        "outputId": "4be36087-2fd4-4f56-bd1d-9dce7ce83448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "foot\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "word = 'feet'\n",
        "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxtyQ22mYi3P",
        "outputId": "375987a1-0f9b-451d-94aa-95d8981428fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['I', 'n'], ['have', 'v'], ['no', 'n'], ['idea', 'n'], ['that', 'n'], ['such', 'a'], ['individual', 'n'], ['exist', 'n'], ['outside', 'n'], ['of', 'n'], ['story', 'n']]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"I had no idea that such individuals exist outside of stories\"\n",
        "\n",
        "\n",
        "print([ [lemmatizer.lemmatize(w, get_wordnet_pos(w)),get_wordnet_pos(w)]  for w in nltk.word_tokenize(sentence)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBsuuuxGYi3P"
      },
      "source": [
        "### SpaCy\n",
        "La biblioteca spaCy es una de las bibliotecas de PNL más populares junto con NLTK. La diferencia básica entre las dos bibliotecas es el hecho de que NLTK contiene una amplia variedad de algoritmos para resolver un problema, mientras que spaCy contiene solo uno, pero el mejor algoritmo para resolver un problema.\n",
        "NLTK se lanzó en 2001, mientras que spaCy es relativamente nuevo y se desarrolló en 2015. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ywrBaSEYi3Q",
        "outputId": "6e38670d-7f52-4482-b804-99feb809502b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.3.0-cp39-cp39-win_amd64.whl (11.6 MB)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.15-cp39-cp39-win_amd64.whl (1.0 MB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy) (3.0.3)\n",
            "Collecting blis<0.8.0,>=0.4.0\n",
            "  Downloading blis-0.7.7-cp39-cp39-win_amd64.whl (6.6 MB)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.7-cp39-cp39-win_amd64.whl (18 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
            "Collecting wasabi<1.1.0,>=0.9.1\n",
            "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.6-cp39-cp39-win_amd64.whl (112 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp39-cp39-win_amd64.whl (448 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
            "Requirement already satisfied: setuptools in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy) (1.22.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.0)\n",
            "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.9.0\n",
            "    Uninstalling pydantic-1.9.0:\n",
            "      Successfully uninstalled pydantic-1.9.0\n",
            "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 wasabi-0.9.1\n",
            "Collecting es-core-news-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.3.0/es_core_news_sm-3.3.0-py3-none-any.whl (12.9 MB)\n",
            "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from es-core-news-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.26.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.22.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.0.15)\n",
            "Requirement already satisfied: setuptools in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (58.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (21.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.62.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: colorama in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.4.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cizai\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.1.0)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.3.0\n",
            "[+] Download and installation successful\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqQT6IbSYi3Q"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWJUw04HYi3Q"
      },
      "outputs": [],
      "source": [
        "text='En el monte de la China una china se perdió y un ruso la encontró'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi94MSmcYi3Q"
      },
      "outputs": [],
      "source": [
        "doc=nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ODxwHigYi3R",
        "outputId": "a48b5f20-573a-4e72-b15b-10438842807b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'la encontró'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[54:65]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vEJ7Nn-Yi3R",
        "outputId": "67367b6e-e0ef-40ac-e467-83cadbd2772c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'En el monte de la China una china se perdió y un ruso la encontró',\n",
              " 'ents': [{'start': 6, 'end': 17, 'label': 'LOC'},\n",
              "  {'start': 18, 'end': 23, 'label': 'LOC'}],\n",
              " 'sents': [{'start': 0, 'end': 53}, {'start': 54, 'end': 65}],\n",
              " 'tokens': [{'id': 0,\n",
              "   'start': 0,\n",
              "   'end': 2,\n",
              "   'tag': 'ADP',\n",
              "   'pos': 'ADP',\n",
              "   'morph': '',\n",
              "   'lemma': 'en',\n",
              "   'dep': 'case',\n",
              "   'head': 2},\n",
              "  {'id': 1,\n",
              "   'start': 3,\n",
              "   'end': 5,\n",
              "   'tag': 'DET',\n",
              "   'pos': 'DET',\n",
              "   'morph': 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art',\n",
              "   'lemma': 'el',\n",
              "   'dep': 'det',\n",
              "   'head': 2},\n",
              "  {'id': 2,\n",
              "   'start': 6,\n",
              "   'end': 11,\n",
              "   'tag': 'NOUN',\n",
              "   'pos': 'NOUN',\n",
              "   'morph': 'Gender=Masc|Number=Sing',\n",
              "   'lemma': 'monte',\n",
              "   'dep': 'obl',\n",
              "   'head': 9},\n",
              "  {'id': 3,\n",
              "   'start': 12,\n",
              "   'end': 14,\n",
              "   'tag': 'ADP',\n",
              "   'pos': 'ADP',\n",
              "   'morph': '',\n",
              "   'lemma': 'de',\n",
              "   'dep': 'case',\n",
              "   'head': 5},\n",
              "  {'id': 4,\n",
              "   'start': 15,\n",
              "   'end': 17,\n",
              "   'tag': 'DET',\n",
              "   'pos': 'DET',\n",
              "   'morph': 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art',\n",
              "   'lemma': 'el',\n",
              "   'dep': 'det',\n",
              "   'head': 5},\n",
              "  {'id': 5,\n",
              "   'start': 18,\n",
              "   'end': 23,\n",
              "   'tag': 'PROPN',\n",
              "   'pos': 'PROPN',\n",
              "   'morph': '',\n",
              "   'lemma': 'China',\n",
              "   'dep': 'nmod',\n",
              "   'head': 2},\n",
              "  {'id': 6,\n",
              "   'start': 24,\n",
              "   'end': 27,\n",
              "   'tag': 'DET',\n",
              "   'pos': 'DET',\n",
              "   'morph': 'Definite=Ind|Gender=Fem|Number=Sing|PronType=Art',\n",
              "   'lemma': 'uno',\n",
              "   'dep': 'det',\n",
              "   'head': 7},\n",
              "  {'id': 7,\n",
              "   'start': 28,\n",
              "   'end': 33,\n",
              "   'tag': 'NOUN',\n",
              "   'pos': 'NOUN',\n",
              "   'morph': 'Gender=Fem|Number=Sing',\n",
              "   'lemma': 'china',\n",
              "   'dep': 'nsubj',\n",
              "   'head': 9},\n",
              "  {'id': 8,\n",
              "   'start': 34,\n",
              "   'end': 36,\n",
              "   'tag': 'PRON',\n",
              "   'pos': 'PRON',\n",
              "   'morph': 'Case=Acc|Person=3|PrepCase=Npr|PronType=Prs|Reflex=Yes',\n",
              "   'lemma': 'él',\n",
              "   'dep': 'expl:pass',\n",
              "   'head': 9},\n",
              "  {'id': 9,\n",
              "   'start': 37,\n",
              "   'end': 43,\n",
              "   'tag': 'VERB',\n",
              "   'pos': 'VERB',\n",
              "   'morph': 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
              "   'lemma': 'perder',\n",
              "   'dep': 'ROOT',\n",
              "   'head': 9},\n",
              "  {'id': 10,\n",
              "   'start': 44,\n",
              "   'end': 45,\n",
              "   'tag': 'CCONJ',\n",
              "   'pos': 'CCONJ',\n",
              "   'morph': '',\n",
              "   'lemma': 'y',\n",
              "   'dep': 'cc',\n",
              "   'head': 12},\n",
              "  {'id': 11,\n",
              "   'start': 46,\n",
              "   'end': 48,\n",
              "   'tag': 'DET',\n",
              "   'pos': 'DET',\n",
              "   'morph': 'Definite=Ind|Gender=Masc|Number=Sing|PronType=Art',\n",
              "   'lemma': 'uno',\n",
              "   'dep': 'det',\n",
              "   'head': 12},\n",
              "  {'id': 12,\n",
              "   'start': 49,\n",
              "   'end': 53,\n",
              "   'tag': 'NOUN',\n",
              "   'pos': 'NOUN',\n",
              "   'morph': 'Gender=Masc|Number=Sing',\n",
              "   'lemma': 'ruso',\n",
              "   'dep': 'conj',\n",
              "   'head': 9},\n",
              "  {'id': 13,\n",
              "   'start': 54,\n",
              "   'end': 56,\n",
              "   'tag': 'PRON',\n",
              "   'pos': 'PRON',\n",
              "   'morph': 'Case=Acc|Gender=Fem|Number=Sing|Person=3|PrepCase=Npr|PronType=Prs',\n",
              "   'lemma': 'él',\n",
              "   'dep': 'obj',\n",
              "   'head': 14},\n",
              "  {'id': 14,\n",
              "   'start': 57,\n",
              "   'end': 65,\n",
              "   'tag': 'VERB',\n",
              "   'pos': 'VERB',\n",
              "   'morph': 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
              "   'lemma': 'encontrar',\n",
              "   'dep': 'ROOT',\n",
              "   'head': 14}]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.to_json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md07OheoYi3R",
        "outputId": "2040744d-fa22-4a9e-c2c4-4b1e15dce7f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'En el monte de la China una china se perdió y un ruso la encontró'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGsijwPyYi3S",
        "outputId": "ff5f4940-0675-483c-ba4c-d938d186ec20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "en True ADP\n",
            "el True DET\n",
            "monte False NOUN\n",
            "de True ADP\n",
            "el True DET\n",
            "China False PROPN\n",
            "uno True DET\n",
            "china False NOUN\n",
            "él True PRON\n",
            "perder False VERB\n",
            "y True CCONJ\n",
            "uno True DET\n",
            "ruso False NOUN\n",
            "él True PRON\n",
            "encontrar False VERB\n"
          ]
        }
      ],
      "source": [
        "for i in doc:\n",
        "    print(i.lemma_, i.is_stop,i.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SFr3CGXYi3T",
        "outputId": "3a5867a7-0c6d-41c2-b42f-41fbf8f4ab7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['texto', 'prueba', 'tokens', 'quedarán', 'normalización']\n"
          ]
        }
      ],
      "source": [
        "def normalize(text):\n",
        "    doc = nlp(text)\n",
        "    words = [t.orth_ for t in doc if not t.is_punct | t.is_stop]\n",
        "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and     \n",
        "    t.isalpha()]\n",
        "    return lexical_tokens\n",
        "word_list = normalize('Soy un texto de prueba. ¿Cuántos tokens me quedarán después de la normalización?')\n",
        "print(word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBNqrERwYi3T",
        "outputId": "917ced31-b5c6-4f60-8a96-14fa70237b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['texto', 'normalmente', 'y', 'grande', 'engañe', 'tamaño']\n",
            "['texto', 'normalmente', 'grande', 'engañe', 'tamaño']\n"
          ]
        }
      ],
      "source": [
        "text = \"Soy un texto. Normalmente soy más largo y más grande. Que no te engañe mi tamaño.\"\n",
        "doc = nlp(text)\n",
        "lexical_tokens = [t.orth_.lower() for t in doc if not t.is_punct | t.is_stop]\n",
        "print(lexical_tokens)\n",
        "print(normalize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ouq0uZL5Yi3T",
        "outputId": "59a5d06a-6e01-4b79-f0ab-f2f6e9364380"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['soy',\n",
              " 'uno',\n",
              " 'texto',\n",
              " 'que',\n",
              " 'pedir',\n",
              " 'a',\n",
              " 'grito',\n",
              " 'que',\n",
              " 'el',\n",
              " 'procesar',\n",
              " '.',\n",
              " 'por',\n",
              " 'ese',\n",
              " 'yo',\n",
              " 'cantar',\n",
              " ',',\n",
              " 'tú',\n",
              " 'cantar',\n",
              " ',',\n",
              " 'él',\n",
              " 'cantar',\n",
              " ',',\n",
              " 'nosotros',\n",
              " 'cantar',\n",
              " ',',\n",
              " 'cantar',\n",
              " ',',\n",
              " 'cantar',\n",
              " '…']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\n",
        "doc = nlp(text)\n",
        "lemmas = [tok.lemma_.lower() for tok in doc]\n",
        "lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z34pPb7sYi3T"
      },
      "source": [
        "## Raices, truncamiento - *stemming*\n",
        "Se llama stemming al procedimiento de convertir palabras en raíces. Estas raíces son la parte invariable de palabras relacionadas sobre todo por su forma. De cierta manera se parece a la lematización, pero los resultados (las raíces) no tienen por qué ser palabras de un idioma. Por ejemplo, el algoritmo de stemming puede decidir que la raíz de amamos no es am- (la raíz que ) sino amam- (cosa que desconcertaría a mas de uno). Aquí va un ejemplo de stemming:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou9DSsc0Yi3U",
        "outputId": "c16440dd-3a2b-49a5-9fe9-c846380052aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['text',\n",
              " 'pid',\n",
              " 'grit',\n",
              " 'proces',\n",
              " 'cant',\n",
              " 'cant',\n",
              " 'cant',\n",
              " 'cant',\n",
              " 'cant',\n",
              " 'cant']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import SnowballStemmer\n",
        "spanishstemmer=SnowballStemmer(\"spanish\")\n",
        "text = \"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\n",
        "tokens = normalize(text) # crear una lista de tokens\n",
        "stems = [spanishstemmer.stem(token) for token in tokens]\n",
        "stems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQjPIHiCYi3U",
        "outputId": "d35cc298-3dc7-4a5f-fbec-519214aec1a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ],
      "source": [
        "print(SnowballStemmer.languages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax1Q6k2qYi3U"
      },
      "source": [
        "## Diferencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_rGVbR2Yi3U",
        "outputId": "0e35bb46-a15d-47b6-d4c5-f9d46f285797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stone\n",
            "speak\n",
            "bedroom\n",
            "joke\n",
            "lisa\n",
            "purpl\n",
            "----------------------\n",
            "stone\n",
            "speaking\n",
            "bedroom\n",
            "joke\n",
            "lisa\n",
            "purple\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer \n",
        "stemmer = PorterStemmer() \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "print(stemmer.stem('stones')) \n",
        "print(stemmer.stem('speaking')) \n",
        "print(stemmer.stem('bedroom')) \n",
        "print(stemmer.stem('jokes')) \n",
        "print(stemmer.stem('lisa')) \n",
        "print(stemmer.stem('purple')) \n",
        "print('----------------------') \n",
        "print(lemmatizer.lemmatize('stones')) \n",
        "print(lemmatizer.lemmatize('speaking')) \n",
        "print(lemmatizer.lemmatize('bedroom')) \n",
        "print(lemmatizer.lemmatize('jokes')) \n",
        "print(lemmatizer.lemmatize('lisa')) \n",
        "print(lemmatizer.lemmatize('purple'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Análisis Morfológico.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}