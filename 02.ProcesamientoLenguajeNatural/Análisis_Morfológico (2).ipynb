{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD_pUQRTYi3B"
      },
      "source": [
        "# Análisis morfológico\n",
        "\n",
        "Es un proceso mediante el cual se examinan las partes que constituyen a las palabras. Implica realizar una disección de los elementos que integran la estructura de una palabra, buscando sus elementos más pequeños. Mediante este proceso se separan los lexemas de los morfemas, se indica entonces la raíz y los modificadores que le acompañan respectivamente.  Apunta mucho a la estructura, una suerte de disección del lenguaje para encontrar la relación y sentido de sus componentes esenciales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7w2bdBRYi3G"
      },
      "source": [
        "## Lematización\n",
        "\n",
        "Consiste en la identificación y asignación de la raíz de una palabra a un lema -una forma canónica de la misma- en forma de etiqueta:\n",
        "\n",
        "> «Lema: forma de citación de una palabra (p. ej., el lema de reía es reir).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN4Ao9XeYi3G"
      },
      "source": [
        "Tenemos diversas herramientas en Python para lemmatizar textos:\n",
        "\n",
        "### WordNet\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O2N6_odYYi3H",
        "outputId": "a9333996-21a8-45e5-dd42-ec6015759d65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        " import nltk\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JdEMzf8fYi3I",
        "outputId": "3d6d4e03-33d3-42df-f674-c88db52f1a20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "someone\n",
            "think\n",
            "in\n",
            "plural\n"
          ]
        }
      ],
      "source": [
        "import nltk #es muy bueno en ingles\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "# Inicialización del lematizador de WordNet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "print(lemmatizer.lemmatize(\"someones\"))\n",
        "print(lemmatizer.lemmatize(\"thinks\"))\n",
        "print(lemmatizer.lemmatize(\"in\"))\n",
        "print(lemmatizer.lemmatize(\"plurals\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "scrolled": true,
        "id": "aB74-POMYi3J",
        "outputId": "4acad5fa-87b2-4d85-c551-7f43f9917c0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'had', 'no', 'idea', 'that', 'such', 'individuals', 'exist', 'outside', 'of', 'stories', '!']\n",
            "I had no idea that such individual exist outside of story !\n"
          ]
        }
      ],
      "source": [
        "sentence = \"I had no idea that such individuals exist outside of stories!\"\n",
        "\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(word_list)\n",
        "\n",
        "#se toma el texto anterior y le corre el lematizador\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "print(lemmatized_output) #las puso en singular las palabras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU7gpMddYi3J"
      },
      "source": [
        "Que ocurre aquí:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HTa22GF4Yi3K",
        "outputId": "46e17763-b1ae-48b8-b816-0f4e162ea38e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n",
            "had\n",
            "my\n",
            "religion\n"
          ]
        }
      ],
      "source": [
        "#verbo = v\n",
        "#sustanttivo= n\n",
        "#advervio = r\n",
        "#adjetivo = a\n",
        "\n",
        "#print(lemmatizer.lemmatize(\"lost\", 'v'))  \n",
        "print(lemmatizer.lemmatize(\"had\", 'v'))  \n",
        "print(lemmatizer.lemmatize(\"had\", 'a'))  \n",
        "print(lemmatizer.lemmatize(\"my\",'n')) \n",
        "print(lemmatizer.lemmatize(\"religions\",'n')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur_CMWiiYi3L",
        "outputId": "e1386303-662c-4f1f-8f87-bdf0602c25de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lost\n",
            "my\n",
            "religion\n"
          ]
        }
      ],
      "source": [
        "print(lemmatizer.lemmatize(\"lost\",'a'))  \n",
        "print(lemmatizer.lemmatize(\"my\",'n')) \n",
        "print(lemmatizer.lemmatize(\"religions\",'n')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU4gKpZGYi3M"
      },
      "outputs": [],
      "source": [
        "print(lemmatizer.lemmatize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0e6NZKYYi3N"
      },
      "source": [
        "Evidentemente debemos usar un tagged para hacer una buena lematización:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WPsOz4nWYi3O"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e1ADNjGQYi3O",
        "outputId": "c0af1466-6ad7-491d-ed82-8358c044f730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'v'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "#la funcion permite identificar si es un verbo, adjetivo, etc\n",
        "get_wordnet_pos('lost')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0f_sAUxYi3P",
        "outputId": "4be36087-2fd4-4f56-bd1d-9dce7ce83448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "foot\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "word = 'feet'\n",
        "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nxtyQ22mYi3P",
        "outputId": "c129b654-b134-4df1-f189-3af96b6cf4d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['I', 'n'], ['have', 'v'], ['no', 'n'], ['idea', 'n'], ['that', 'n'], ['such', 'a'], ['individual', 'n'], ['exist', 'n'], ['outside', 'n'], ['of', 'n'], ['story', 'n']]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"I had no idea that such individuals exist outside of stories\"\n",
        "\n",
        "\n",
        "print([ [lemmatizer.lemmatize(w, get_wordnet_pos(w)),get_wordnet_pos(w)]  for w in nltk.word_tokenize(sentence)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBsuuuxGYi3P"
      },
      "source": [
        "### SpaCy\n",
        "La biblioteca spaCy es una de las bibliotecas de PNL más populares junto con NLTK. La diferencia básica entre las dos bibliotecas es el hecho de que NLTK contiene una amplia variedad de algoritmos para resolver un problema, mientras que spaCy contiene solo uno, pero el mejor algoritmo para resolver un problema.\n",
        "NLTK se lanzó en 2001, mientras que spaCy es relativamente nuevo y se desarrolló en 2015. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1ywrBaSEYi3Q",
        "outputId": "c1db2ece-d425-487c-8fe0-517dc1b72494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "2022-08-20 00:18:15.696523: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mqQT6IbSYi3Q"
      },
      "outputs": [],
      "source": [
        "#los modelos pueden ser multilenguaje\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')#paquete de español"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "mWJUw04HYi3Q"
      },
      "outputs": [],
      "source": [
        "text='''En el monte de la china una china se perdió y un ruso la encontró.\n",
        "El ruso la llamó y le dijo: !Hola¡\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Oi94MSmcYi3Q"
      },
      "outputs": [],
      "source": [
        "doc=nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "XhsqEPmesbeY",
        "outputId": "fb8ca63d-2648-4a7f-ead0-4b0aa4f86660"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'En el monte de la china una china se perdió y un ruso la encontró.\\nEl ruso la llamó y le dijo: !Hola¡\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in doc.sents:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXCI-L-2sfya",
        "outputId": "2b5dd2c7-c37b-49ef-9353-16b599bf343c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En el monte de la china una china se perdió y un ruso la encontró.\n",
            "\n",
            "El ruso la llamó y le dijo: !Hola¡\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in doc:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv-eYtrps9XJ",
        "outputId": "4c4357b7-19d7-4d1e-e85a-04d5808ce9cf"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En\n",
            "el\n",
            "monte\n",
            "de\n",
            "la\n",
            "china\n",
            "una\n",
            "china\n",
            "se\n",
            "perdió\n",
            "y\n",
            "un\n",
            "ruso\n",
            "la\n",
            "encontró\n",
            ".\n",
            "\n",
            "\n",
            "El\n",
            "ruso\n",
            "la\n",
            "llamó\n",
            "y\n",
            "le\n",
            "dijo\n",
            ":\n",
            "!\n",
            "Hola\n",
            "¡\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "5ODxwHigYi3R",
        "outputId": "baa38762-41e9-4a6f-f1bd-7ca3ae7dc513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'china'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "text[18:23]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[28:33]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uXUH8cFWozTX",
        "outputId": "6f9fa4a3-9c16-4294-c544-6e2308e0efdd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'china'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "-vEJ7Nn-Yi3R",
        "outputId": "1abd5373-2016-4e20-f184-c1512991f50c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'En el monte de la china una china se perdió y un ruso la encontró.\\nEl ruso la llamó y le dijo: !Hola¡\\n',\n",
              " 'ents': [],\n",
              " 'sents': [{'start': 0, 'end': 67}, {'start': 67, 'end': 102}],\n",
              " 'tokens': [{'id': 0,\n",
              "   'start': 0,\n",
              "   'end': 2,\n",
              "   'tag': 'ADP',\n",
              "   'pos': 'ADP',\n",
              "   'morph': '',\n",
              "   'lemma': 'en',\n",
              "   'dep': 'case',\n",
              "   'head': 2},\n",
              "  {'id': 1,\n",
              "   'start': 3,\n",
              "   'end': 5,\n",
              "   'tag': 'DET',\n",
              "   'pos': 'DET',\n",
              "   'morph': 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art',\n",
              "   'lemma': 'el',\n",
              "   'dep': 'det',\n",
              "   'head': 2},\n",
              "  {'id': 2,\n",
              "   'start': 6,\n",
              "   'end': 11,\n",
              "   'tag': 'NOUN',\n",
              "   'pos': 'NOUN',\n",
              "   'morph': 'Gender=Masc|Number=Sing',\n",
              "   'lemma': 'monte',\n",
              "   'dep': 'obl',\n",
              "   'head': 9},\n",
              "  {'id': 3,\n",
              "   'start': 12,\n",
              "   'end': 14,\n",
              "   'tag': 'ADP',\n",
              "   'pos': 'ADP',\n",
              "   'morph': '',\n",
              "   'lemma': 'de',\n",
              "   'dep': 'case',\n",
              "   'head': 5},\n",
              "  {'id': 4,\n",
              "   'start': 15,\n",
              "   'end': 17,\n",
              "   'tag': 'DET',\n",
              "   'pos': 'DET',\n",
              "   'morph': 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art',\n",
              "   'lemma': 'el',\n",
              "   'dep': 'det',\n",
              "   'head': 5},\n",
              "  {'id': 5,\n",
              "   'start': 18,\n",
              "   'end': 23,\n",
              "   'tag': 'NOUN',\n",
              "   'pos': 'NOUN',\n",
              "   'morph': 'Gender=Fem|Number=Sing',\n",
              "   'lemma': 'china',\n",
              "   'dep': 'nmod',\n",
              "   'head': 2},\n",
              "  {'id': 6,\n",
              "   'start': 24,\n",
              "   'end': 27,\n",
              "   'tag': 'DET',\n",
              "   'pos': 'DET',\n",
              "   'morph': 'Definite=Ind|Gender=Fem|Number=Sing|PronType=Art',\n",
              "   'lemma': 'uno',\n",
              "   'dep': 'det',\n",
              "   'head': 7},\n",
              "  {'id': 7,\n",
              "   'start': 28,\n",
              "   'end': 33,\n",
              "   'tag': 'NOUN',\n",
              "   'pos': 'NOUN',\n",
              "   'morph': 'Gender=Fem|Number=Sing',\n",
              "   'lemma': 'china',\n",
              "   'dep': 'nsubj',\n",
              "   'head': 9},\n",
              "  {'id': 8,\n",
              "   'start': 34,\n",
              "   'end': 36,\n",
              "   'tag': 'PRON',\n",
              "   'pos': 'PRON',\n",
              "   'morph': 'Case=Acc|Person=3|PrepCase=Npr|PronType=Prs|Reflex=Yes',\n",
              "   'lemma': 'él',\n",
              "   'dep': 'expl:pass',\n",
              "   'head': 9},\n",
              "  {'id': 9,\n",
              "   'start': 37,\n",
              "   'end': 43,\n",
              "   'tag': 'VERB',\n",
              "   'pos': 'VERB',\n",
              "   'morph': 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
              "   'lemma': 'perder',\n",
              "   'dep': 'ccomp',\n",
              "   'head': 14},\n",
              "  {'id': 10,\n",
              "   'start': 44,\n",
              "   'end': 45,\n",
              "   'tag': 'CCONJ',\n",
              "   'pos': 'CCONJ',\n",
              "   'morph': '',\n",
              "   'lemma': 'y',\n",
              "   'dep': 'cc',\n",
              "   'head': 12},\n",
              "  {'id': 11,\n",
              "   'start': 46,\n",
              "   'end': 48,\n",
              "   'tag': 'DET',\n",
              "   'pos': 'DET',\n",
              "   'morph': 'Definite=Ind|Gender=Masc|Number=Sing|PronType=Art',\n",
              "   'lemma': 'uno',\n",
              "   'dep': 'det',\n",
              "   'head': 12},\n",
              "  {'id': 12,\n",
              "   'start': 49,\n",
              "   'end': 53,\n",
              "   'tag': 'NOUN',\n",
              "   'pos': 'NOUN',\n",
              "   'morph': 'Gender=Masc|Number=Sing',\n",
              "   'lemma': 'ruso',\n",
              "   'dep': 'conj',\n",
              "   'head': 9},\n",
              "  {'id': 13,\n",
              "   'start': 54,\n",
              "   'end': 56,\n",
              "   'tag': 'PRON',\n",
              "   'pos': 'PRON',\n",
              "   'morph': 'Case=Acc|Gender=Fem|Number=Sing|Person=3|PrepCase=Npr|PronType=Prs',\n",
              "   'lemma': 'él',\n",
              "   'dep': 'obj',\n",
              "   'head': 14},\n",
              "  {'id': 14,\n",
              "   'start': 57,\n",
              "   'end': 65,\n",
              "   'tag': 'VERB',\n",
              "   'pos': 'VERB',\n",
              "   'morph': 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
              "   'lemma': 'encontrar',\n",
              "   'dep': 'ROOT',\n",
              "   'head': 14},\n",
              "  {'id': 15,\n",
              "   'start': 65,\n",
              "   'end': 66,\n",
              "   'tag': 'PUNCT',\n",
              "   'pos': 'PUNCT',\n",
              "   'morph': 'PunctType=Peri',\n",
              "   'lemma': '.',\n",
              "   'dep': 'punct',\n",
              "   'head': 14},\n",
              "  {'id': 16,\n",
              "   'start': 66,\n",
              "   'end': 67,\n",
              "   'tag': 'SPACE',\n",
              "   'pos': 'SPACE',\n",
              "   'morph': '',\n",
              "   'lemma': '\\n',\n",
              "   'dep': 'dep',\n",
              "   'head': 15},\n",
              "  {'id': 17,\n",
              "   'start': 67,\n",
              "   'end': 69,\n",
              "   'tag': 'DET',\n",
              "   'pos': 'DET',\n",
              "   'morph': 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art',\n",
              "   'lemma': 'el',\n",
              "   'dep': 'det',\n",
              "   'head': 18},\n",
              "  {'id': 18,\n",
              "   'start': 70,\n",
              "   'end': 74,\n",
              "   'tag': 'NOUN',\n",
              "   'pos': 'NOUN',\n",
              "   'morph': 'Gender=Masc|Number=Sing',\n",
              "   'lemma': 'ruso',\n",
              "   'dep': 'nsubj',\n",
              "   'head': 20},\n",
              "  {'id': 19,\n",
              "   'start': 75,\n",
              "   'end': 77,\n",
              "   'tag': 'PRON',\n",
              "   'pos': 'PRON',\n",
              "   'morph': 'Case=Acc|Gender=Fem|Number=Sing|Person=3|PrepCase=Npr|PronType=Prs',\n",
              "   'lemma': 'él',\n",
              "   'dep': 'obj',\n",
              "   'head': 20},\n",
              "  {'id': 20,\n",
              "   'start': 78,\n",
              "   'end': 83,\n",
              "   'tag': 'VERB',\n",
              "   'pos': 'VERB',\n",
              "   'morph': 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
              "   'lemma': 'llamar',\n",
              "   'dep': 'ROOT',\n",
              "   'head': 20},\n",
              "  {'id': 21,\n",
              "   'start': 84,\n",
              "   'end': 85,\n",
              "   'tag': 'CCONJ',\n",
              "   'pos': 'CCONJ',\n",
              "   'morph': '',\n",
              "   'lemma': 'y',\n",
              "   'dep': 'cc',\n",
              "   'head': 23},\n",
              "  {'id': 22,\n",
              "   'start': 86,\n",
              "   'end': 88,\n",
              "   'tag': 'PRON',\n",
              "   'pos': 'PRON',\n",
              "   'morph': 'Case=Dat|Number=Sing|Person=3|PronType=Prs',\n",
              "   'lemma': 'él',\n",
              "   'dep': 'iobj',\n",
              "   'head': 23},\n",
              "  {'id': 23,\n",
              "   'start': 89,\n",
              "   'end': 93,\n",
              "   'tag': 'VERB',\n",
              "   'pos': 'VERB',\n",
              "   'morph': 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
              "   'lemma': 'decir',\n",
              "   'dep': 'conj',\n",
              "   'head': 20},\n",
              "  {'id': 24,\n",
              "   'start': 93,\n",
              "   'end': 94,\n",
              "   'tag': 'PUNCT',\n",
              "   'pos': 'PUNCT',\n",
              "   'morph': 'PunctType=Colo',\n",
              "   'lemma': ':',\n",
              "   'dep': 'punct',\n",
              "   'head': 26},\n",
              "  {'id': 25,\n",
              "   'start': 95,\n",
              "   'end': 96,\n",
              "   'tag': 'PUNCT',\n",
              "   'pos': 'PUNCT',\n",
              "   'morph': 'PunctSide=Fin|PunctType=Excl',\n",
              "   'lemma': '!',\n",
              "   'dep': 'punct',\n",
              "   'head': 26},\n",
              "  {'id': 26,\n",
              "   'start': 96,\n",
              "   'end': 100,\n",
              "   'tag': 'PROPN',\n",
              "   'pos': 'PROPN',\n",
              "   'morph': '',\n",
              "   'lemma': 'Hola',\n",
              "   'dep': 'ccomp',\n",
              "   'head': 23},\n",
              "  {'id': 27,\n",
              "   'start': 100,\n",
              "   'end': 101,\n",
              "   'tag': 'PUNCT',\n",
              "   'pos': 'PUNCT',\n",
              "   'morph': 'PunctSide=Ini|PunctType=Excl',\n",
              "   'lemma': '¡',\n",
              "   'dep': 'punct',\n",
              "   'head': 26},\n",
              "  {'id': 28,\n",
              "   'start': 101,\n",
              "   'end': 102,\n",
              "   'tag': 'SPACE',\n",
              "   'pos': 'SPACE',\n",
              "   'morph': '',\n",
              "   'lemma': '\\n',\n",
              "   'dep': 'dep',\n",
              "   'head': 27}]}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "doc.to_json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Md07OheoYi3R",
        "outputId": "10b1e382-c9bc-4765-c4fe-bbf06fda1b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'En el monte de la china una china se perdió y un ruso la encontró.\\nEl ruso la llamó y le dijo: !Hola¡\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "doc.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in doc:\n",
        "  print(i,i.pos_,i.lemma_,i.morph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5VDwz24ttOp",
        "outputId": "b3d0e9ef-b705-4aa9-bdfa-c94c958235de"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En ADP en \n",
            "el DET el Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
            "monte NOUN monte Gender=Masc|Number=Sing\n",
            "de ADP de \n",
            "la DET el Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
            "china NOUN china Gender=Fem|Number=Sing\n",
            "una DET uno Definite=Ind|Gender=Fem|Number=Sing|PronType=Art\n",
            "china NOUN china Gender=Fem|Number=Sing\n",
            "se PRON él Case=Acc|Person=3|PrepCase=Npr|PronType=Prs|Reflex=Yes\n",
            "perdió VERB perder Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
            "y CCONJ y \n",
            "un DET uno Definite=Ind|Gender=Masc|Number=Sing|PronType=Art\n",
            "ruso NOUN ruso Gender=Masc|Number=Sing\n",
            "la PRON él Case=Acc|Gender=Fem|Number=Sing|Person=3|PrepCase=Npr|PronType=Prs\n",
            "encontró VERB encontrar Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
            ". PUNCT . PunctType=Peri\n",
            "\n",
            " SPACE \n",
            " \n",
            "El DET el Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
            "ruso NOUN ruso Gender=Masc|Number=Sing\n",
            "la PRON él Case=Acc|Gender=Fem|Number=Sing|Person=3|PrepCase=Npr|PronType=Prs\n",
            "llamó VERB llamar Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
            "y CCONJ y \n",
            "le PRON él Case=Dat|Number=Sing|Person=3|PronType=Prs\n",
            "dijo VERB decir Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
            ": PUNCT : PunctType=Colo\n",
            "! PUNCT ! PunctSide=Fin|PunctType=Excl\n",
            "Hola PROPN Hola \n",
            "¡ PUNCT ¡ PunctSide=Ini|PunctType=Excl\n",
            "\n",
            " SPACE \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "sGsijwPyYi3S",
        "outputId": "7fcc3ba3-677b-4e27-996f-2aa866d1b429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en True ADP\n",
            "el True DET\n",
            "monte False NOUN\n",
            "de True ADP\n",
            "el True DET\n",
            "china False NOUN\n",
            "uno True DET\n",
            "china False NOUN\n",
            "él True PRON\n",
            "perder False VERB\n",
            "y True CCONJ\n",
            "uno True DET\n",
            "ruso False NOUN\n",
            "él True PRON\n",
            "encontrar False VERB\n",
            ". False PUNCT\n",
            "\n",
            " False SPACE\n",
            "el True DET\n",
            "ruso False NOUN\n",
            "él True PRON\n",
            "llamar False VERB\n",
            "y True CCONJ\n",
            "él True PRON\n",
            "decir True VERB\n",
            ": False PUNCT\n",
            "! False PUNCT\n",
            "Hola False PROPN\n",
            "¡ False PUNCT\n",
            "\n",
            " False SPACE\n"
          ]
        }
      ],
      "source": [
        "for i in doc:\n",
        "    print(i.lemma_, i.is_stop,i.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "8SFr3CGXYi3T",
        "outputId": "d7a0772b-4834-4eed-966b-b29f075e9803",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['texto', 'prueba', 'tokens', 'quedarán', 'normalización']\n"
          ]
        }
      ],
      "source": [
        "#quita las palabras vacias y los signos de puntuación \n",
        "\n",
        "def normalize(text):\n",
        "    doc = nlp(text)\n",
        "    words = [t.orth_ for t in doc if not t.is_punct | t.is_stop]\n",
        "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and     \n",
        "    t.isalpha()]\n",
        "    return lexical_tokens\n",
        "word_list = normalize('Soy un texto de prueba. ¿Cuántos tokens me quedarán después de la normalización?')\n",
        "print(word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "LBNqrERwYi3T",
        "outputId": "946ed7b0-f23a-4345-a376-3200a7d1aca3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['texto', 'normalmente', 'engañe', 'tamaño']\n",
            "['texto', 'normalmente', 'engañe', 'tamaño']\n"
          ]
        }
      ],
      "source": [
        "#no se recomienda utilzar letras capitalizadas, o mayusculas y o minusculas\n",
        "\n",
        "text = \"Soy un texto. Normalmente soy más largo y más grande. Que no te engañe mi tamaño.\"\n",
        "doc = nlp(text)\n",
        "lexical_tokens = [t.orth_.lower() for t in doc if not t.is_punct | t.is_stop]\n",
        "print(lexical_tokens)\n",
        "print(normalize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Ouq0uZL5Yi3T",
        "outputId": "dbc24509-a4ba-4db3-8bfd-eb83f075ab2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ser',\n",
              " 'uno',\n",
              " 'texto',\n",
              " 'que',\n",
              " 'pedir',\n",
              " 'a',\n",
              " 'grito',\n",
              " 'que',\n",
              " 'él',\n",
              " 'procesen',\n",
              " '.',\n",
              " 'por',\n",
              " 'ese',\n",
              " 'yo',\n",
              " 'canto',\n",
              " ',',\n",
              " 'tú',\n",
              " 'canta',\n",
              " ',',\n",
              " 'él',\n",
              " 'cantar',\n",
              " ',',\n",
              " 'yo',\n",
              " 'cantar',\n",
              " ',',\n",
              " 'cantái',\n",
              " ',',\n",
              " 'cantar',\n",
              " '…']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "text = \"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\n",
        "doc = nlp(text)\n",
        "lemmas = [tok.lemma_.lower() for tok in doc]\n",
        "lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z34pPb7sYi3T"
      },
      "source": [
        "## Raices, truncamiento - *stemming*\n",
        "Se llama stemming al procedimiento de convertir palabras en raíces. Estas raíces son la parte invariable de palabras relacionadas sobre todo por su forma. De cierta manera se parece a la lematización, pero los resultados (las raíces) no tienen por qué ser palabras de un idioma. Por ejemplo, el algoritmo de stemming puede decidir que la raíz de amamos no es am- (la raíz que ) sino amam- (cosa que desconcertaría a mas de uno). Aquí va un ejemplo de stemming:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ou9DSsc0Yi3U",
        "outputId": "7058dcac-956d-495b-c982-1919545c9ad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text',\n",
              " 'pid',\n",
              " 'grit',\n",
              " 'proces',\n",
              " 'cant',\n",
              " 'cant',\n",
              " 'cant',\n",
              " 'cant',\n",
              " 'cant',\n",
              " 'cant',\n",
              " 'cantidad']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import SnowballStemmer\n",
        "spanishstemmer=SnowballStemmer(\"spanish\")\n",
        "text = \"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan… de las cantidades\"\n",
        "tokens = normalize(text) # crear una lista de tokens\n",
        "stems = [spanishstemmer.stem(token) for token in tokens]\n",
        "stems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQjPIHiCYi3U",
        "outputId": "d35cc298-3dc7-4a5f-fbec-519214aec1a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ],
      "source": [
        "print(SnowballStemmer.languages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax1Q6k2qYi3U"
      },
      "source": [
        "## Diferencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_rGVbR2Yi3U",
        "outputId": "0e35bb46-a15d-47b6-d4c5-f9d46f285797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stone\n",
            "speak\n",
            "bedroom\n",
            "joke\n",
            "lisa\n",
            "purpl\n",
            "----------------------\n",
            "stone\n",
            "speaking\n",
            "bedroom\n",
            "joke\n",
            "lisa\n",
            "purple\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer \n",
        "stemmer = PorterStemmer() \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "print(stemmer.stem('stones')) \n",
        "print(stemmer.stem('speaking')) \n",
        "print(stemmer.stem('bedroom')) \n",
        "print(stemmer.stem('jokes')) \n",
        "print(stemmer.stem('lisa')) \n",
        "print(stemmer.stem('purple')) \n",
        "print('----------------------') \n",
        "print(lemmatizer.lemmatize('stones')) \n",
        "print(lemmatizer.lemmatize('speaking')) \n",
        "print(lemmatizer.lemmatize('bedroom')) \n",
        "print(lemmatizer.lemmatize('jokes')) \n",
        "print(lemmatizer.lemmatize('lisa')) \n",
        "print(lemmatizer.lemmatize('purple'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Análisis_Morfológico.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}